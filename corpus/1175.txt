 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  You're given an array that is almost sorted, in that each of the elements may be misplaced by no more than positions from the correct sorted order. Find a space-and-time efficient algorithm to sort the array.  I have an solution as follows.  Let's denote to mean the elements of the array from index (inclusive) to (exclusive).  Sort  Now we know that are in their final sorted positions...  ...but may still be misplaced by !  Sort  Now we know that are in their final sorted positions...  ...but may still be misplaced by  Sort  ....  Until you sort , then you're done!  This final step may be cheaper than the other steps when you have less than elements left  In each step, you sort at most elements in , putting at least elements in their final sorted positions at the end of each step. There are steps, so the overall complexity is .  My questions are:  Is optimal? Can this be improved upon?  Can you do this without (partially) re-sorting the same elements?  358k123123 gold badges550550 silver badges615615 bronze badges  4  9  I wonder if you couldn't take advantage of the fact that after Step 1, [k..2k) are in sorted relative to each other? So instead of sorting [k..3k), you could sort [2k..4k) and merge the last half of the 1st ([k..2k)) with the first half of the second ([2k..3k)).  Yes it is optimal. Simple proof it meets the lower bound is we randomly permute each block of k elements: (k)(k)(k)(k). Thus we need to do N/k sorts each taking k*log(k). Can we do it without resorting elements? Yes. As above sort each block of k elements independently. Then serially go over and in place merge block i with block i+1. Except on boundaries we can also do the merges independently in parallel. Thanks for the question. This algorithm is actually useful for a problem I am working on :)  @Chad: In fact Moron gives an even simpler proof: consider the possibility that k = n. Then an algorithm faster than O(n log k) would contradict the known optimality of O(n log n) algorithms for comparison-based sorting.  5 Answers 5  As Bob Sedgewick showed in his dissertation work (and follow-ons), insertion sort absolutely crushes the "almost-sorted array". In this case your asymptotics look good but if k < 12 I bet insertion sort wins every time. I don't know that there's a good explanation for why insertion sort does so well, but the place to look would be in one of Sedgewick's textbooks entitled Algorithms (he has done many editions for different languages).  I have no idea whether O(N log k) is optimal, but more to the point, I don't really care—if k is small, it's the constant factors that matter, and if k is large, you may as well just sort the array.  Insertion sort will nail this problem without re-sorting the same elements.  Big-O notation is all very well for algorithm class, but in the real world, constants matter. It's all too easy to lose sight of this. (And I say this as a professor who has taught Big-O notation!)  192k5858 gold badges349349 silver badges529529 bronze badges  11  6  Can you explain more of what he said instead of just linking to it? References in answers are awesome, but substantive content on stackoverflow itself is even awesomer!  Well, even in the real world, when the input sizes grow large enough, the asymptotics matter more than the constants. :-) Insertion sort has a very good constant, but the fact that O(n log k) is asymptotically better than O(nk) can matter — for example, what if k ≈ √n as n grows large? (It also depends on what the interviewer was looking for. :p)  @Norman: Perhaps you can actually point to the paper/book chapter which has the claim about almost sorted arrays? Just a link to the homepage is practically useless. Also, just saying insertion sort will nail it is useless, if k = sqrt(n) for instance. I really don't understand why this answer has so many votes.  – Aryabhatta  Apr 28 '10 at 23:47  3  @Moron: If k = log n then k is small. log base 2 of one million is just 20. @Everyone: SO is a programming site, not a CS theory site!  While SO is a programming site, I think questions still deserve correct answers. For example, we ought not to say that all algorithms are O(1), even though in programming all running times encountered are bounded by a constant (like 10^1000). More to the point here, whatever the constant of insertion sort, there is some sufficiently large k after which insertion sort is no longer faster, and we cannot "may as well" sort the whole array. (I really doubt, even with a trillion elements (k=40) whether insertion sort is faster.)  If using only the comparison model, O(n log k) is optimal. Consider the case when k = n.  To answer your other question, yes it is possible to do this without sorting, by using heaps.  Use a min-heap of 2k elements. Insert 2k elements first, then remove min, insert next element etc.  This guarantees O(n log k) time and O(k) space and heaps usually have small enough hidden constants.  +1. I also came up with the min-heap approach (can't you just limit the size to instead of ?), and was told to improve it so that it doesn't use the extra space.  @polygenelubricants: You can do this in-place. Start from the far end, and use a max-heap instead of a min-heap. Heapify that final block of 2k elements in-place. Store the first extracted element in a variable; subsequent elements go in the positions vacated immediately before the final block of 2k (which contains the heap structure), similar to regular heapsort. When only 1 block remains, heapsort it in place. A final O(n) pass is needed to "rotate" the final block back to the initial block. The rotation is not trivial but can be done in O(n) and O(1) space.  @polygenelubricants: Strange I seem to have missed your comment to this answer! @j_random_hacker: Seems right.  – Aryabhatta  Jul 11 '10 at 16:42  BTW @Moron, I really like your argument that O(n log k) is optimal: "Consider k = n". Doesn't get much simpler than that!  @j_random_hacker Can you explain why the heap has to be of size 2k? In the examples I've done k+1 is big enough.  It was already pointed out that one of the asymptotically optimal solutions uses a min heap and I just wanted to provide code in Java:  Since is apparently supposed to be pretty small, an insertion sort is probably the most obvious and generally accepted algorithm.  In an insertion sort on random elements, you have to scan through N elements, and you have to move each one an average of N/2 positions, giving ~N*N/2 total operations. The "/2" constant is ignored in a big-O (or similar) characterization, giving O(N2) complexity.  In the case you're proposing, the expected number of operations is ~N*K/2 -- but since is a constant, the whole term is ignored in a big-O characterization, so the overall complexity is O(N).  448k7474 gold badges586586 silver badges10491049 bronze badges  1  2  is not guaranteed a constant, so this is really . If is a constant, though, you're right it's .  Your solution is a good one if is large enough. There is no better solution in terms of time complexity; each element might be out of place by places, which means you need to learn bits of information to place it correctly, which means you need to make comparisons at least--so it's got to be a complexity of at least .  However, as others have pointed out, if is small, the constant terms are going to kill you. Use something that's very fast per operation, like insertion sort, in that case.  If you really wanted to be optimal, you'd implement both methods, and switch from one to the other depending on .  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  