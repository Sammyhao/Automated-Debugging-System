 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  When using a min/max-heap algorithm, priorities may change. One way to handle this is to removal and insert the element to update the queue order.  For priority queues implemented using arrays, this can be a performance bottleneck that seems avoidable, especially for cases when the change to priority is small.  Are there well known best-practice methods for updating elements in the min/max-heap?  Background Info: I'm no expert in binary-trees, I inherited some code that has a performance bottleneck re-inserting elements in a priority queue. I've made a re-insertion function for the min-heap that re-orders the new element - which gives a measurable improvement over (remove & insert), however this seems the kind of problem that others may have solved in a more elegant way.  I could link to the code if it helps but would rather not focus too much on implementation details - since this Q&A can probably be kept general.  33.8k2828 gold badges160160 silver badges273273 bronze badges  6  Is there any particular pattern to how the priorities change? Also, how frequent are priority changes relative to other operations like insertions and deletions?  Priorities change many times more than insertion or removal, the whole purpose of the min-heap (in this case), is to track the changes while popping the best. Although not sure this is relevant to the question? (even if it wasn't the bottleneck - it may be helpful to optimize for this case)  When the priorities change, is there any pattern to it? Like, do they always increase, always decrease, etc.?  No, although they might not change by much (re-ordering might not even be needed), this seems like something that could be optimized for. Even if it was following some patterns/constraints, I think that might be a different question.  2 Answers 2  Typical Solution  The usual solution is to mark an element as invalid and insert a new element, then eliminate the invalid entries as they are popped-off.  Alternative Solution  If that approach doesn't suffice, it is possible restore the min-heap invariant in O(log n) steps as long as the location of the value being changed is known.  Recall that min-heaps are built and maintained using two primitives, "siftup" and "siftdown" (though various sources have differing opinions on which is up and which is down). One of those pushes values down the tree and the other floats them upward.  Case 1: Value is increased  If the new value x1 is greater than the old value x0, then only the tree under x needs to be fixed because . Just push x down the tree by swapping x with the smaller of its two children while x is bigger than one of its children.  Case 2: Value is decreased  If the new value x1 is less than the old value x, the tree below x needs no adjustment because . Instead, we just need to move upward, swapping x with its parent while x is less than its parent. Sibling nodes need not be considered because they are already greater than or equal to a parent which will potentially be replaced with a lower value.  Case 3: Value is unchanged  No work is necessary. The existing invariants are unchanged.  Working code in Python  Test 1,000,000 trials: Create a random heap. Alter a randomly selected value. Restore the heap condition. Verify that the result is a min-heap.  193k5656 gold badges335335 silver badges434434 bronze badges  8  2  Interesting & good to know, although I'm not sure I could use this in my case since elements are updated frequently on a large data-set, which could give a large and unpredictable worst-case outcome. Maybe a detail - but I'm using fixed size allocations, so the possibility to re-alloc potentially large arrays isn't so appealing.  Great comprehensive answer (updated since my last comment), I'm using this in production code and it's given a significant speedup.  I've never seen the "typical solution" used. The asymptotic complexity of both solutions is O(n) if you don't know where the item is in the heap. If you do know where the item is, then the "typical solution" is kind of silly because, as you've shown, re-adjusting the heap is trivial.  @jim-mischel, right, not to mention the penalty for having unused nodes in the heap which need to be sifted through when performing any changes to the heap. Are there good examples of open-source heap API's which support re-adjusting? (I'm writing my own but am interested to see how others do this in production code).  I don't know if any such APIs that are public. I wrote my own in C# several years ago. It used a Dictionary to keep track of items' positions in the heap.  These functions can run on the modified element, depending on the change relative to the current value. eg:  While the number of operations depends on the distribution of values, this will be equal or fewer steps then maintaining a sorted list.  In general, small changes are _much_ more efficient than a remove+insert.  See working code, and test, which implements a min-heap with insert/remove/re-prioritize, without an initial lookup (the caller stores an opaque reference).  Even re-ordering only the required elements may end up being many operations for a large heap.  If this is too inefficient, a minimum heap may not a a good fit.  A binary-tree might be better (red-black tree for example), where removal and insertion scale better.  However I'm not sure about an rb-trees ability to re-order in-place, as a min-heap can do.  33.8k2828 gold badges160160 silver badges273273 bronze badges  3  6  The expensive part of changing the priority of an item in a binary heap is finding the item's location in the heap. That's typically an O(n) operation, unless you have a secondary data structure that keeps track of the index of every item. Maintaining that data structure requires that you update it every time you move an item, but it allows you to locate an item in O(1). Changing the priority after you've located the item requires at most O(log n) steps. As you've noted, the actual number of steps required depends, in general, on the magnitude of the change.  This depends on the implementation, check the code linked - users of the API store an opaque reference to the node, allowing for fast removal and re-prioritization. In fact there is no search function (which would indeed be slower especially if it isn't balanced).  Yes, it depends on the implementation. Their heap item node contains the index value. That qualifies as a secondary data structure that keeps track of the index, and puts additional burden on the client to dereference the node to get to their actual data. But it does work well. A binary heap is always balanced, but it's not ordered for efficient search, so any implementation that doesn't maintain that index makes changing priority or removing arbitrary nodes rather expensive.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  