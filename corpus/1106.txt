 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I'm training vanilla RNN in PyTorch to learn the changes in the hidden dynamics.  There's no problem with forward pass and bk prop for the initial batch but when it comes to the part where I use prev. hidden state as initial state it's somehow considered as in-place operation.  I don't really understand why this makes a problem and how to solve it.  I tried to follow the given hint by setting in but no progress.  1 Answer 1  This is possibly due to you clearing the gradients with optim.zero_grad(). Something like the following should work. You can try detaching tensors to do something like TBPTT with tensor.detach() if memory becomes an issue.  EDIT: Upon closer inspection, you also have the same code running if batch is not 0. This creates a new optimizer and a new every time it is run.  Thanks for pointing out to this but I need to pass to initially then afterwards. How can I do this without re-defining func and optimizer ?  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  