 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  The code below will print me the highest frequency it can find in my hash table (of which is a bunch of linked lists) 10 times. I need my code to print the top 10 frequencies in my hash table. I do not know how to do this (code examples would be great, plain english logic/pseudocode is just as great).  I create a temporary hashing list called 'tmp' which is pointing to my hash table 'hashtable'  A while loop then goes through the list and looks for the highest frequency, which is an int 'tmp->freq'  The loop will continue this process of duplicating the highest frequency it finds with the variable 'topfreq' until it reaches the end of the linked lists on the the hash table.  My 'node' is a struct comprising of the variables 'freq' (int) and 'word' (128 char). When the loop has nothing else to search for it prints these two values on screen.  The problem is, I can't wrap my head around figuring out how to find the next lowest number from the number I've just found (and this can include another node with the same freq value, so I have to check that the word is not the same too).  44k1616 gold badges110110 silver badges128128 bronze badges  asked May 10 '09 at 5:19  user77482user77482  3  1  Please change your tabs to 4 spaces. That hurts the eyes, and the goggles do nothing! ;)  7 Answers 7  Keep an array of 10 node pointers, and insert each node into the array, maintaining the array in sorted order. The eleventh node in the array is overwritten on each iteration and contains junk.  I would maintain a set of words already used and change the inner-most if condition to test for frequency greater than previous top frequency AND tmp->word not in list of words already used.  When iterating over the hash table (and then over each linked list contained therein) keep a self balancing binary tree (std::set) as a "result" list. As you come across each frequency, insert it into the list, then truncate the list if it has more than 10 entries. When you finish, you'll have a set (sorted list) of the top ten frequencies, which you can manipulate as you desire.  There may be perform gains to be had by using sets instead of linked lists in the hash table itself, but you can work that out for yourself.  As one of the requirements I'm not able to use a binary tree. Thank you very much for your suggestion, it was very appreciated!  – user77482  May 10 '09 at 5:49  @tmhai: One of the "requirements" is not being allowed to use a binary tree? Is this homework by any chance? (If so, fine, but tag your question as such.)  Move the vector into a sorted container via insertion sort, but insert into a container (e.g. linkedlist or vector) of size 10, and drop any elements that fall off the bottom of the list.  Step 2 (Efficient):  Same as step 1, but keep track of the size of the item at the bottom of the list, and skip the insertion step entirely if the current item is too small.  Suppose there are n words in total, and we need the most-frequent k words (here, k = 10).  If n is much larger than k, the most efficient way I know of is to maintain a min-heap (i.e. the top element has the minimum frequency of all elements in the heap). On each iteration, you insert the next frequency into the heap, and if the heap now contains k+1 elements, you remove the smallest. This way, the heap is maintained at a size of k elements throughout, containing at any time the k highest-frequency elements seen so far. At the end of processing, read out the k highest-frequency elements in increasing order.  Time complexity: For each of n words, we do two things: insert into a heap of size at most k, and remove the minimum element. Each operation costs O(log k) time, so the entire loop takes O(nlog k) time. Finally, we read out the k elements from a heap of size at most k, taking O(klog k) time, for a total time of O((n+k)log k). Since we know that k < n, O(klog k) is at worst O(nlog k), so this can be simplified to just O(nlog k).  A hash table containing linked lists of words seems like a peculiar data structure to use if the goal is to accumulate are word frequencies.  Nonetheless, the efficient way to get the ten highest frequency nodes is to insert each into a priority queue/heap, such as the Fibonacci heap, which has O(1) insertion time and O(n) deletion time. Assuming that iteration over the hash table table is fast, this method has a runtime which is O(n×O(1) + 10×O(n)) ≡ O(n).  +1. Two things: Deletion time for all heaps that I know of (including Fibonacci) is O(log n), not O(n); and if n >> 10 it will be much faster to keep a min-heap (rather than a max-heap), to which you insert the next element and remove the smallest on each iteration (so that the heap only ever contains at most 10 elements).  O(log(n)) ≡ O(n) is its amortized deletion time - since this algorithm involves only deleting some of the keys an amortized analysis isn’t valid. The Fibonacci heap’s worst case deletion is O(n).  – cweider  May 10 '09 at 18:57  Good point. So in the general case, when we want the top k frequencies, your algorithm will be O(kn) -- i.e. the same as the obvious solution of running the first k iterations of a selection sort. See my answer for an O(nlog(k)) algo that uses a plain binary heap (rather than a Fibonacci heap, which is trickier to implement).  The absolute fastest way to do this would be to use a SoftHeap. Using a SoftHeap, you can find the top 10 items in O(n) time whereas every other solution posted here would take O(n lg n) time.  This wikipedia article shows how to find the median in O(n) time using a softheap, and the top 10 is simply a subset of the median problem. You could then sort the items that were in the top 10 if you needed them in order, and since you're always at most sorting 10 items, it's still O(n) time.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  