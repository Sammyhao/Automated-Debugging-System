 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I am trying to run a word count example. My current testing setup is: NameNode and ResourceManager on one machine (10.38.41.134). DataNode and NodeManager on another (10.38.41.135). They can ssh between them without passwords.  When reading the logs, I don't get any warnings, except a security warning (I didn't set it up for testing) and a containermanager.AuxServices 'mapreduce_shuffle' warning. Upon submitting the example job, nodes react to it and output logs, which suggests that they can communicate well. NodeManager outputs memory usage, but the job doesn't budge.  Where should I even start looking for problems? Everything else I could find is either old or non-relevant. I followed the official cluster setup tutorial for version 2.5.1 which left way too many questions unanswered.  I doubt it's the memory. The NodeManager reports memory usage around 240MB out of 2GB, and sticks there. I'm even trying to run the simplest example with the smallest input, which shouldn't be memory intensive.  1 Answer 1  I suggest you first try to get it working with a single server cluster so it's easier to debug. When that is working, continue with two nodes.  As already suggested, memory might be an issue. Without tweaking the settings, it seems some 2GB is the minimum and I'd recommend at least 4GB per server. Also remember to check also the job's logs (under logs/userlogs, especially syslog).  P.S. I share your frustration about old / non-relevant documentation.  Thanks for the tips. I doubt it's the memory, because I'm trying to run the simplest example. The job's logs proved useful. I'm thrilled now because I am getting error messages!!! :D Seems like somewhere in the mess I have configured a wrong address. Wherever I saw the RM's address, I put it to 10.38.41.134, as it really is in my case. Which .xml file governs the job's settings?  It seems you are missing entry yarn.resourcemanager.scheduler.address (which uses port 8030 by default) in yarn-site.xml. Also I believe that mapreduce.jobtracker.address is for old version of MR - use yarn.resourcemanager.resource-tracker.address instead  It might be too early to say it, but I love you nevertheless. The job failed miraculously, but IT FAILED, it didn't get stuck, it went from the start to the end and this is the happiest fail of my life. I've been going over the .xmls for a week already. Working on it from home, working on it overtime... The pieces are starting to fall together now, with all the confs and daemons. THANK YOU!  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  