 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I have a data set with 130000 elements and I have two different data structures which are a doubly-linked list and hash table. When inserting data set elements into the linked list, I place the node at the end of the list by using tail pointer. When inserting data set elements into hash table , I benefit from open addressing method with a probe function. I face 110000 collisions for last 10 elements in the data set.  However, difference between total running times of insertion for two different data structures is equal to 0.0981 second.  Linked list = 0.028521 second  Hash Table = 0.120102 second  Is pointer operation slow or probing method very fast ?  79511 gold badge66 silver badges1717 bronze badges  3  2  If your have 110000 collisions for 10 elements, then something is wrong. There is a limit on how full a hash table can get... How large is your hash table? How do you probe? But the problem could also be the hash function (less likely).  Personally what suits me best in most scenarios is a cache-friendly separate chaining implementation using singly-linked lists. "Cache-friendly" and "linked list" might seem like oxymorons but you can allocate the list nodes in a way such that they are generally contiguous. For example, the linked list might be implemented with the list pointers just being 32-bit indices into an array (that's actually how I do it). If you have room for a post-process, you can even reconstruct the hash table in a way such that each neighboring node in a bucket is guaranteed to be contiguous.  – user4842163  Dec 7 '17 at 10:48  It's not necessarily theoretically optimal but I find it the easiest way to achieve a performant solution without putting much thought into it. The hash table itself is also just an array of bucket head indices which index a node with a index. The list nodes are stored in one giant array (, i.e.). The reason I like this solution is that the memory use is very predictable. It's 4 bytes per bucket and 4 bytes per node inserted.  2 Answers 2  Insertion at the end of Double Linked List via the tail pointer is O(1), as you read here too.  Insertion in a Hash Table with Open Addressing can be also constant, as you can read here too.  However, a correct and efficient implementation of a Hash Table with Open Addressing is quite tricky, since a lot of things can wrong (probing, load factor, hash function, etc.). Even Wikipedia mentions that..  I face 110000 collisions (in the hash table) for last 10 elements  This is a strong indication that something is not good in your hash table implementation.  This explains why the time measurements you made, if they are correct, make the double linked list faster than your hash table.  Is pointer operation slow or probing method very fast ?  There are no actual algorithms, so the answer will be rather theoretical.  In terms of performance, the general answer is: cache misses are expensive. Having DDR with 60 ns latency and 3.2 GHz CPU, last level cache miss stalls CPU for 60 * 3.2 =~ 200 cycles.  Double-linked List  For double linked list algorithm, you have to access tail pointer, tail element and new element. If you just add elements in a loop, there is a huge probability all those accesses will be in CPU cache.  In real life application, if you do something between the additions, you might have up to 3 cache misses (tail pointer, tail element, new element).  Hash Table  For hash table with open addressing the situation is a bit different. The hash function produces a random index in hash table, so usually, the first access to a hash table is a cache miss. In your case, the hash table is not that big (130K pointers), so it might fit into the L3 cache. But still, L3 cache miss is about 30 cycles CPU stall.  But what happens next? You just put a pointer into the table, no need to update tail element nor the new element. So no cache misses here.  If the hash table element is occupied, you just check the next one. Such a sequential access is easily predictable by CPU prefetcher, so all those accesses usually do no produce any cache misses as well: CPU prefetches next hash table into L1 cache.  So, in real application, hash table usually has one cache miss, but since the hash is unpredictable, hash table always have this first cache miss.  The Answer  To get a practical answer, what is going on in you application, you should use a tool to analyse CPU performance counters, like on Linux or on Windows. The tool will show what exactly your CPU spend time on.  Real Application  One more theoretical disclaimer here. I guess, that if you fix your hash table (say, use few elements per bucket rather than open addressing) and effectively reduce number of collision, the performance could be on par.  Should you use double-linked list over the hash table or vice versa? It depend on you application. Hash table is good for random access, i.e. you can access any element in O(1) time. For double linked list you have to traverse the list, so the estimation would be O(n).  On the other hand, just adding elements at the end of the list is not only a cheaper operation, but also much easier to implement. You do not care about any collisions and hash table overflows.  So, in some cases double linked list would have huge advantages over the hash table and it is up to the application what will suit the best.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  