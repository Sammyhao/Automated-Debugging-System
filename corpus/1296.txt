 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I've searched for some time now and none of the solutions seem to work for me.  Pretty straightforward - I want to upload data from my local file system to HDFS using the Java API. The Java program will be run on a host that has been configured to talk to a remote Hadoop cluster through shell (i.e. , etc.).  I have included the below dependencies in my project:  I have code that looks like the following:  The local data is not being copied to the Hadoop cluster, but no errors are reported and no exceptions are thrown. I've enabled logging for the package. I see the following outputs:  Can anyone assist in helping me resolve this issue?  EDIT 1: (09/15/2015)  I've removed 2 of the Hadoop dependencies - I'm only using one now:  My code is now the following:  I was previously executing my application with the following command:  Now I'm executing it with this command:  With these changes, my application now interacts with HDFS as intended. To my knowledge, the command is meant only for Map Reduce jobs packaged as an executable jar, but these changes did the trick for me.  hadoop command include hadoop classes in the classpath and if it is necessary unjar the jar and repackage it to include some libraries. At the enh the haddop command will execute the java command but with the extra objects/configurations.  I've already packaged my program as a fat jar using the Maven Shade plugin. Were you referring to something else? If so, please clarify.  3 Answers 3  i am not sure about the approach you are following, but below is one way data can be uploaded to hdfs using java libs :  Also if you have hadoop conf xmls locally, you can include them in you class path. Then hadoop fs details will automatically be picked up at runtime, and you will not need to set "fs.defaultFS" . Also if you are running in old hdfs version you might need to use "fs.default.name" instead of "fs.defaultFS". If you are not sure of the hdfs endpoint, it is usually the hdfs namenode url . Here is example from previous similar question copying directory from local system to hdfs java code  My program is packaged as a fat jar using the Maven Shade plugin. I notice that if I try to execute the jar with the program doesn't execute as intended, but if I execute the jar with it does what I intended. Do you know why?  If you review the method in the class, you will notice that several locations are being included in the classpath.  Then, if you are executing your class directly using the java -jar command you could be ignoring all the other required steps to execute your job in hadoop that hadoop jar are doing.  Thanks for the clarification. Let me take a step back for a second: should I even be using the command to run my program? My program does interact with the HDFS, but it isn't a Map Reduce job. From my understanding, the command is for running Map Reduce jobs. It just so happens that my program's code gets execute properly with this command (as opposed to the command) because certain other things are added to the classpath.  If your project doesn't require to run in a distributed environment (MapReduce or Yarn) then it is NOT necessary to execute it using hadoop jar. You only need to be sure that the hadoop-client dependency is included in your project. For me, the easy way to do it is using maven to manage the dependencies. executing the project with the statement:  Yeah, thanks for the suggestion. After switching to , I ended up getting an about the and/or not being set. Am I missing some extra configuration that needs to occur now that I'm using instead of ?  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  