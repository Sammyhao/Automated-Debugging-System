 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  Wrote this program to unzip and extract files to amazon S3. I have ran into a java heap bug.  Things I've tried: Increase heap space on the arguments. Change byte size to [1024*1024]  Getting a bug at outputStream.write(buffer, 0, len). The byte size is initialized at 1024. This code works for most files up to size 166 mb so far. Java heap size is at Xmx4096m Java version 1.7  1 Answer 1  Do you really need the ? . It looks like you only use it to get the uncompressed size, but you already have it at . Could you pass the directly to ?  As for the actual issue you are observing, when reaching those levels of memory consumption it is not unusual to run into fragmentation issues. That is, while you have more free memory than requested, you do not have a contiguous chunk as large and so the allocation fails. A compacting garbage collector should take care of that, but not all garbage collectors in the JVM are compacting, IIRC.  20.9k44 gold badges3434 silver badges4848 bronze badges  1  I found that by increasing the Java heap space in the run command versus java console fixed the problem. But I think your right still, i think that byte array is useless in is filling up with unnecessary wasted memory. I will revisit this code but currently it is working up to 1gb sized files but I bet it could be a lot better.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  