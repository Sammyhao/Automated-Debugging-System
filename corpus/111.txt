 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  If we construct sorted set based on RB Tree and Heap. Do insertion() and deleteMax() for n times.  (1). What's the Big-O ?  My idea: For both RB tree and heap , deleteMax() and insertion() will all take nlog(n), so does it mean the time complexity will be O(nlog(n) + nlog(n)) = O(2nlog(n)) ?  (2). How about the difference of their constant factors.  Just a note: Linux used to have a scheduler called CFS. It used a red-black tree instead of a priority queue, and it had good performance.  1 Answer 1  Assume you are talking about binary heap. For each insertion/maxDeletion, the time complexity is for both RB and binary heap, where is the number of existing elements in the RB/heap. Fibonacci heap has better theoretical time complexity. You should read the wiki.  As to the constant, binary heap is better than RB. It is also much easier to implement and takes less space. You should use heap when you only want to keep track of the min/max without requiring to know the complete ordering of all elements.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  