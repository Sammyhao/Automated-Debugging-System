 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I need to write function which loads dictionary in hash table.I'm confused about error message: c:37:20 runtime error: load of null pointer of type 'const char', which runs in segmentation fault.  I've tried to change load function, but still didn`t help. And also tried to allocate memory for hashtable, as I thought problem might be in memory leaks.  The function load should retun true when dictionary is loaded. But I get segmentation fault. Does it mean I didn't get right output from load function?  Sorry if there are any mistakes in my question.I'm using stack for first time)))) You can see it right in code return tolower(word[0]) - 'a'; // this is error line 37:20  Additionally returns a value that you'd probably want to store in a variable and use later yet I don't see any evidence to it - instead you discard the return value and use later.  Here is definition of struct node and hashtable // Represents number of buckets in a hash table #define N 26 maybe I malloc memoryfor hashtable in a wrong way? // Represents a node in a hash table typedef struct node { char word[LENGTH + 1]; struct node *next; } node; // Represents a hash table node *hashtable[N];  1 Answer 1  you do not use the result of hash() and you use i rather than the hash result as index in hashtable, if N greater than 26 you read/write out of hashtable, in the other case you do not put the word in the right entry because the first at index 0, the next at index 1 etc whatever their first letter  Note is never true and in fact never reach because is always true because you limit the number of word to read  Must be something like that doing minimal changes  but in fact can be simplified to be :  Note I suppose you do not read several times the same word (it is a dictionary)  To do  is dangerous because there is no protection if the read word is longer than LENGTH  supposing LENGTH is 32 do ( the word can store 32 characters more the final null character) :  There is no reason to have the loop :  remove it (but not its body of course), so :  tte part  is useless because hashtable being global is initialized with 0  If you want to reload the dictionary you need to free the linked list before to reset to NULL  memory leaks  the malloc in hash is useless and only create memory leaks, remove it :  Warning if the first letter is not a-z or A-Z the return index is not a valid index for hashtable  32k77 gold badges2323 silver badges3636 bronze badges  5  Thanks a lot, bruno! But hashtable has 26 buckets only(leters of alphabet in ASCII,and than I'm supposed to use linked list(where I insert my nodes) And thatt undarstand how to link nodes and hashtable bucket,which is array element, not pointer  @ksju yes, you implement a kind of hash table, so you have synonymous and to use a linked list for the synonymous is classical. If you take into account what I say in my answer it must be ok, and you are able to read any number of words (you are not limited to 26)  @ksju I missed to say you need to remove the (but not its body), because I suppose the goal if to only reference the words by their first letter  I also missed to say that this part of code is CS 50 task, and some code is defalt, as hash function, node and hashtable declaration, for example, and I can't change it formula(hashing only first letter). but I'm confused about fact that error messege is in their (CS50) code not mine  @ksju no problem except the naming hash which doesn't not really correspond to the goal of the function. If you do the changes I speak about your program must work. I edited my answer to add a full definition more execution examples  Post as a guest  Not the answer you're looking for? Browse other questions tagged c or ask your own question.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  