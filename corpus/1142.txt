 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  What's the best way to wait (without spinning) until something is available in either one of two (multiprocessing) Queues, where both reside on the same system?  9 Answers 9  Actually you can use multiprocessing.Queue objects in select.select. i.e.  would select que only if it is ready to be read from.  No documentation about it though. I was reading the source code of the multiprocessing.queue library (at linux it's usually sth like /usr/lib/python2.6/multiprocessing/queue.py) to find it out.  With Queue.Queue I didn't have found any smart way to do this (and I would really love to).  This works great on Unix, but on Windows the implementation can only deal with sockets, not file descriptors and therefore this fails.  What's the main difference between and , and can be used for multithreading and not just multiprocessing?  @CMCDragonkai I think queue.Queue is a data structure of blocking queue; the synchronization around queue.Queue depends on Mutex. There is no file descriptor or anything similar beneath queue.Queue, so we cannot OS system calls like select, epoll, kqueue, to wait for it.  It doesn't look like there's an official way to handle this yet. Or at least, not based on this:  You could try something like what this post is doing -- accessing the underlying pipe filehandles:  Not sure how well the select on a multiprocessing queue works on windows. As select on windows listens for sockets and not file handles, I suspect there could be problems.  My answer is to make a thread to listen to each queue in a blocking fashion, and to put the results all into a single queue listened to by the main thread, essentially multiplexing the individual queues into a single one.  Seems like using threads which forward incoming items to a single Queue which you then wait on is a practical choice when using multiprocessing in a platform independent manner.  Avoiding the threads requires either handling low-level pipes/FDs which is both platform specific and not easy to handle consistently with the higher-level API.  Or you would need Queues with the ability to set callbacks which i think are the proper higher level interface to go for. I.e. you would write something like:  Maybe the multiprocessing package could grow this API but it's not there yet. The concept works well with py.execnet which uses the term "channel" instead of "queues", see here http://tinyurl.com/nmtr4w  18.4k33 gold badges3939 silver badges4646 bronze badges  3  That would be a very nice interface! (Though clearly there's benefit to keeping the stdlib interfaces tight, as Jesse mentions in the @ars' referenced bug report.)  true but the current Queue public API doesn't handle your use case which i think is a common one.  If it's "common" - file a bug report + patch (with tests for the love of pete) on bugs.python.org and I can evaluate it for 2.7/3.x  You could use something like the Observer pattern, wherein Queue subscribers are notified of state changes.  In this case, you could have your worker thread designated as a listener on each queue, and whenever it receives a ready signal, it can work on the new item, otherwise sleep.  Well, the is destructive, so you can't really do observation on the queue itself as GoF describe it. The dequeue-ing thread would have to be the "observed" -- I was hoping for less overhead than two additional threads.  Also, if I wanted a single point of access for the calling process (like in ) I would need a thread-safe queue on top of those two threads.  Not sure how well the select on a multiprocessing queue works on windows. As select on windows listens for sockets and not file handles, I suspect there could be problems.  My answer is to make a thread to listen to each queue in a blocking fashion, and to put the results all into a single queue listened to by the main thread, essentially multiplexing the individual queues into a single one.  My code for doing this is:  The follow code is my test routine to show how it works:  The one situation where I'm usually tempted to multiplex multiple queues is when each queue corresponds to a different type of message that requires a different handler. You can't just pull from one queue because if it isn't the type of message you want, you need to put it back.  However, in this case, each handler is essentially a separate consumer, which makes it an a multi-producer, multi-consumer problem. Fortunately, even in this case you still don't need to block on multiple queues. You can create different thread/process for each handler, with each handler having its own queue. Basically, you can just break it into multiple instances of a multi-producer, single-consumer problem.  The only situation I can think of where you would have to wait on multiple queues is if you were forced to put multiple handlers in the same thread/process. In that case, I would restructure it by creating a queue for my main thread, spawning a thread for each handler, and have the handlers communicate with the main thread using the main queue. Each handler could then have a separate queue for its unique type of message.  Put a header on the messages and send them to a common queue. This simplifies the code and will be cleaner overall.  Imagine we have different handlers for different message types in a system. If they are all reading from the same queue, then they need to put back any messages that aren't for them. By splitting the queues we don't have this concurrency issue.  In my situation, stepping back and reconsidering helped me make the code cleaner and robuster. I had two producers fighting over one consumer. Having only one queue for it was the right thing to do.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  