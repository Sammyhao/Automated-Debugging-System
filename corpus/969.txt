 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  We have used HashMaps in someplaces for storing some data.From logs we can I identify that its reproducing at the same place.  I wanted to ask if Garbage Collector spends more time in clearing up the hashmaps?  Upon looking at the code( i cant share here ), I have found that that there is a Hashmap created like  but this hashmap is never used. Is this a kind of memory leak in my application ?  If this is created inside a method which is doing some processing and it is not used elsewhere also this method is accessed my multiple threads say 20 .Then in such a case would it impact,creating Hashmap as above, Garbage collector to spend more time in recovering heap and throw OOME.  If the reference doesn't escape the scope in which it was declared then no, it does not contribute to the creation of a memory leak.  Will garbage collector face problems in clearing these Hashmaps?If suppose there are large number of these hashmaps  Use a profiler to look at memory consumption or increase you maximum memory size. If you don't measure your program you are just guessing. Using lots and lots of HashMaps is not good for performance/memory but it may not be your biggest problem.  I think you need to have a look at the contents of the heap so you can see where the leak is happening. In a big system it's generally really hard to find a leak from reading the code. Memory analysis tools will at least point you in the right direction.  @DaveHowes I collected the heapDump on OOME and found that LinkedBlockingQueue that we have used in the ThreadpoolExecutor is occupying the 93% of the total heap allocated to the process.total heap allocated is -Xmx3500m.Is this happening because the tasks are produced much faster than the threads in the pool executing it?Could this java bug would be responsible for this : bugs.sun.com/view_bug.do?bug_id=6806875 .How to handle such a situation ??  3 Answers 3  n one of our java application we have got OutOfMemoryError:GC Overhead limit exceeded. We have used HashMaps in someplaces for storing some data.From logs we can I identify that its reproducing at the same place.  If the Hashmap is simply ever building and most likely marked as static, which means you keep adding things to this hashmap and never delete. Then one fine day it will lead to OutOfMemoryError.  I wanted to ask if Garbage Collector spends more time in clearing up the hashmaps?  Garbage collector spends time on the objects which are not referenced, weakly referenced, soft referenced. Wherever it find such objects, depending on the need it will clear them.  Upon looking at the code( i cant share here ), I have found that that there is a Hashmap created like Hashmap topo = new HashMap(); , but this hashmap is never used. Is this a kind of memory leak in my application ?  if this Hashmap is created inside a method which is doing some processing and it is not used elsewhere also this method is accessed my multiple threads say 20 . Then in such a case would it impact,creating Hashmap as above, Garbage collector to spend more time in recovering heap and throw OOME.  If it is hashmap local to a methid, and the method exits after doing some processing, then it should be garbage collected as soon as method exits. As the hashmap is local to the method, so each thread will have a separate copy of this map and once thread finishes the method execution, map is eligible for GC.  65.2k1111 gold badges9090 silver badges127127 bronze badges  3  Thanks...Suppose if there are large number of such unused Hashmaps , Will garbage Collector spend some time in recovering Heap and is it possible that it will throw OOME.  @Saurav Simple references of empty hashmaps will hardly eat substantial heap space.  Short answer is, you're leaking elsewhere. You will be holding or accumulating actual data for the lifetime of the application.. a transitory HashMap, local to the method & not referenced outside, will not be the problem.  You need to look for long-lifetime objects & structures, which might be the actual problem, rather than wildly grasping at some clueless manager's idea of potential problem.  Look out especially for static/ or application-lifetime Maps or Lists, which are added to during the lifetime rather than just at initialization. It will most likely be one, or several, of these that are accumulating.  Note also that inner classes (Listeners, Observers) can capture references to their containing scope & prevent these from being GC'ed indefinitely.  13.5k33 gold badges5555 silver badges7373 bronze badges  4  I collected the heapDump on OOME and found that LinkedBlockingQueue that we have used in the ThreadpoolExecutor is occupying the 93% of the total heap allocated to the process.total heap allocated is -Xmx3500m.Is this happening because the tasks are produced much faster than the threads in the pool executing it?Could this java bug would be responsible for this : bugs.sun.com/view_bug.do?bug_id=6806875 .How to handle such a situation ??  That JDK bug shouldn't cause OOME -- it should just degrade performance and cause "full GC" when a "minor GC" would otherwise be sufficient.  Why don't you wrap the entry (addition) and exit (removal) points for the LinkedBlockingQueue, and count how many tasks in/out and queued? Log this & see if it's the case, and -- if so -- then you can look into why so many tasks are enqueued.  In order to solve the problem, i have used ArrayBlockingQueue(50) and there are max of 20 threads in the threadpool.Since I have used a bounded queue the subsequent task will get rejected so used RejectedExecutionHandler to handle the rejected tasks and default handler policy CallerRunsPolicy since i dont want to lose the task.I am looking forward to test this solution.Please provide your comments.Will it be helpful ?  You need some more details. You need to profile your application to see what objects are consuming the heap space.  Then, if some of the sizeable objects are no longer actually being used by your application, you have a memory leak. Look at the references to these objects to find out why they're still being held in memory when they're no longer useful, and then modify your code to no longer hold these references.  Alternatively, you may find that all of the objects in memory are what you would expect as your working set. Then either you need to increase the heap size, or refactor your application to work with a smaller working set (e.g. streaming events one at a time rather than reading an entire list; storing the last seesion details in the database rather than memory; etc.).  99.2k3333 gold badges186186 silver badges225225 bronze badges  7  I collected the heapDump on OOME and found that LinkedBlockingQueue that we have used in the ThreadpoolExecutor is occupying the 93% of the total heap allocated to the process.total heap allocated is -Xmx3500m.Is this happening because the tasks are produced much faster than the threads in the pool executing it?Could this java bug would be responsible for this : bugs.sun.com/view_bug.do?bug_id=6806875 .How to handle such a situation ??  @Saurav Again, this is something that you'll have to look at, but it seems likely that you're adding tasks too fast. Are the tasks in that queue all pending execution? If so, then indeed you've generated 3.3GB worth of data that needs to be run in future but hasn't been yet. There's no simple answer in this case; either you throttle the speed at which tasks are created, increase the speed that they're processed, or increase the heap size. Which one you choose will depend on your requirements and the details of your situation.  In order to solve the problem, i have used ArrayBlockingQueue(50) and there are max of 20 threads in the threadpool.Since I have used a bounded queue the subsequent task will get rejected so used RejectedExecutionHandler to handle the rejected tasks and default handler policy CallerRunsPolicy since i dont want to lose the task.I am looking forward to test this solution.Please provide your comments.Will it be helpful ?  @Saurav That really depends on your situation. The issue seems to be that you're generating tasks faster than they can be executed, which (if you don't lose any) will always cause you to run out of memory eventually. You need to throttle the speed that tasks are created - and "tying up" a producer by making them run the task may do this. However I don't think it's ideal for two reasons...  @Saurav -- Account for the mismatched number/rate of tasks being generated & consumed first. Your problem is probably there. Otherwise you're putting a bucket under a leak, without checking why there's a hole in the roof. First rule of fixing: understand the problem properly & identify the actual problem. Your queue's not the problem, why you're putting so much stuff in is.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  