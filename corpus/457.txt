 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  48.1k4444 gold badges143143 silver badges201201 bronze badges  10  16  This sounds very interesting. I'd love if someone could post some code that generates this.  I simply found the problem, that lead to too much memory-usage, near to the limit of the heap. A simple solution could be simply to give some more Heap-memory to the Java-Engine (-Xmx) but this only helps, if the application needs exactly as much memory, as the heap-limit before was set.  @SimonKuang Note that there are multiple scenarios for which increasing the heap isn't a valid solution: running out of native threads and running out of perm gen (which is separate from heap) are two examples. Be careful about making overly broad statements about ; there's an unexpectedly diverse set of things that can cause them.  22 Answers 22  This message means that for some reason the garbage collector is taking an excessive amount of time (by default 98% of all CPU time of the process) and recovers very little memory in each run (by default 2% of the heap).  This effectively means that your program stops doing any progress and is busy running only the garbage collection at all time.  To prevent your application from soaking up CPU time without getting anything done, the JVM throws this so that you have a chance of diagnosing the problem.  The rare cases where I've seen this happen is where some code was creating tons of temporary objects and tons of weakly-referenced objects in an already very memory-constrained environment.  Check out the Java GC tuning guide, which is available for various Java versions and contains sections about this specific problem:  Java 11 tuning guide has dedicated sections on excessive GC for different garbage collectors:  there is no mention of this specific error condition for the Garbage First (G1) collector.  288k5555 gold badges533533 silver badges596596 bronze badges  14  10  Would it be correct to summarise your answer as follows: "It's just like an 'Out of Java Heap space' error. Give it more memory with -Xmx." ?  @Tim: No, that wouldn't be correct. While giving it more memory could reduce the problem, you should also look at your code and see why it produces that amount of garbage and why your code skims just below the "out of memory" mark. It's often a sign of broken code.  Thanks, it seems Oracle isn't actually that good in data migration, they broke the link.  @Guus: if multiple applications run in the same JVM, then yes, they can easily influence each other. It'll be hard to tell which one is misbehaving. Separating the applications into distinct JVMs might be the easiest solution.  @TimCooper - that's honestly a poor answer even for the Out of Java Heap space error, though it's certainly sometimes necessary. To trigger this Error, however, you really have to be beating up the JVM, it's quite good at efficiently collecting garbage. If you're seeing this error, it is far more likely you're doing something violently cruel to the JVM than it is that you're simply overloading the heap.  The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option to the command line.  EDIT: looks like someone can type faster than me :)  Can you tell me the difference between "-XX" and "-Xmx"? I was able to turn it off using the "-Xmx" option too.  Replying to a very old comment here, but... @Bart The at the start of several command line options is a flag of sorts indicating that this option is highly VM-specific and unstable (subject to change without notice in future versions). In any case, the flag tells the VM to disable GC overhead limit checking (actually "turns it off"), whereas your command merely increased the heap. In the latter case the GC overhead checking was still running, it just sounds like a bigger heap solved the GC thrashing issues in your case (this will not always help).  In my application (reading a large Excel file in Talend) this did not work and from other users explanation I understand why. This just disables the error but the problem persists and your application will just spend most of its time handling GC. Our server had plenty of RAM so I used the suggestions by Vitalii to increase the heap size.  You will eventually get this error if your application is data intensive, clearing the memory and evading data leak is the best way out - but requires some time.  If you are sure there are no memory leaks in your program, try to:  Increase the heap size, for example .  Enable the concurrent low pause collector .  Reuse existing objects when possible to save some memory.  If necessary, the limit check can be disabled by adding the option to the command line.  102k2626 gold badges145145 silver badges111111 bronze badges  2  10  I disagree with the third advice. Reuse existing objects do not save memory (do not leak old objects save memory :-) Moreover "reuse existing object" was a practice to relieve GC pressure. But it's NOT ALWAYS a good idea: with modern GC, we should avoid situations where old objects hold new ones because it can break some locality assumptions...  @mcoolive: For a somewhat contrived example, see the comments to answer stackoverflow.com/a/5640498/4178262 below; creating the object inside the loop caused GC to be called 39 times instead of 22 times.  Now granted, this is not the best test or the best design but when faced with a situation where you have no choice but implementing such a loop or when dealing with existing code that behaves badly, choosing to reuse objects instead of creating new ones can reduce the number of times the garbage collector gets in the way...  1,34011 gold badge1212 silver badges1717 bronze badges  3  13  Please clarify: When you say "Triggered n times", does that mean that a regular GC happened n times, or that the "GC overhead limit exceeded" error reported by the OP happened n times?  I tested just now using java 1.8.0_91 and never got an error/exception, and the "Triggered n times" was from counting up the number of lines in the file. My tests show much fewer times overall, but fewest "Triggers" times for BETTER, and now, BAD is "badder" than WORST now. My counts: BAD: 26, WORSE: 22, BETTER 21.  I just added a "WORST_YET" modification where I define the in the outer loop instead of before the outer loop, and Triggered 39 garbage collections.  [...] "GC overhead limit exceeded" indicates that the garbage collector is running all the time and Java program is making very slow progress.  After a garbage collection, if the Java process is spending more than approximately 98% of its time doing garbage collection and if it is recovering less than 2% of the heap and has been doing so far the last 5 (compile time constant) consecutive garbage collections, then a is thrown. [...]  Increase the heap size if current heap is not enough.  If you still get this error after increasing heap memory, use memory profiling tools like MAT ( Memory analyzer tool), Visual VM etc and fix memory leaks.  Upgrade JDK version to latest version ( 1.8.x) or at least 1.7.x and use G1GC algorithm. . The throughput goal for the G1 GC is 90 percent application time and 10 percent garbage collection time  Apart from setting heap memory with - , try  Have a look at some more related questions regarding G1GC  @JPerk the max is as much as the physical memory of your machine. However, other applications will compete over memory use if you try that.  4,54511 gold badge2323 silver badges2323 bronze badges  1  Works great for the simulator. Any idea how this affects real devices? i.e. is this a good idea or is it just masking the issue? Thanks.  How could you even think this is a solution to his question in general? You set your heap size to 4g which is totally arbitrary in a gradle configuration for Android facepalm.  You can also increase memory allocation and heap size by adding this to your file:  It doesn't have to be 2048M and 32g, make it as big as you want.  Sets the initial size of the Java heap. The default size is 2097152 (2MB). The values must be a multiple of, and greater than, 1024 bytes (1KB). (The -server flag increases the default size to 32M.)  Sets the initial Java heap size for the Eden generation. The default value is 640K. (The -server flag increases the default size to 2M.)  Sets the maximum size to which the Java heap can grow. The default size is 64M. (The -server flag increases the default size to 128M.) The maximum heap limit is about 2 GB (2048MB).  Java memory arguments (xms, xmx, xmn) formatting  When setting the Java heap size, you should specify your memory argument using one of the letters “m” or “M” for MB, or “g” or “G” for GB. Your setting won’t work if you specify “MB” or “GB.” Valid arguments look like this:  -Xms64m or -Xms64M -Xmx1g or -Xmx1G Can also use 2048MB to specify 2GB Also, make sure you just use whole numbers when specifying your arguments. Using -Xmx512m is a valid option, but -Xmx0.5g will cause an error.  I'm working in Android Studio and encountered this error when trying to generate a signed APK for release. I was able to build and test a debug APK with no problem, but as soon as I wanted to build a release APK, the build process would run for minutes on end and then finally terminate with the "Error java.lang.OutOfMemoryError: GC overhead limit exceeded". I increased the heap sizes for both the VM and the Android DEX compiler, but the problem persisted. Finally, after many hours and mugs of coffee it turned out that the problem was in my app-level 'build.gradle' file - I had the 'minifyEnabled' parameter for the release build type set to 'false', consequently running Proguard stuffs on code that hasn't been through the code-shrinking' process (see https://developer.android.com/studio/build/shrink-code.html). I changed the 'minifyEnabled' parameter to 'true' and the release build executed like a dream :)  In short, I had to change my app-level 'build.gradle' file from: //...  To increase heap size in IntelliJ IDEA follow the following instructions. It worked for me.  For Windows Users,  Go to the location where IDE is installed and search for following.  you can try to make changes on the server setting by referring to this image and increase the memory size for processing process changes highlighted in yellow  you can also make changes to java heap by opening cmd-> 2g(2gigabytes) depending upon the complexity of your program  try to use less constant variable and temp variables  These settings are only specific to your local IDE. This will no work for Prod environment.  In Netbeans, it may be helpful to design a max heap size. Go to Run => Set Project Configuration => Customise. In the Run of its popped up window, go to VM Option, fill in . It could solve heap size problem.  I don't know if this is still relevant or not, but just want to share what worked for me.  @Buhb I reproduced this by this in an normal spring-boot web application within its main method. Here is the code:  The sample code that caused an come is also from oracle java8 language specifications.  I got this error while working with Oracle web logic server. I am sharing my answer for reference in case someone end up here looking for the solution.  So, if you are trying to up the Oracle web logic server and got this error then you just have to increase the initial and maximum heap size set for running the server.  Go to - > C:\Oracle\Middleware\Oracle_Home\user_projects\domains\wl_server\bin  open setDomainEnv.cmd  check set USER_MEM_ARGS value , if its less then  set USER_MEM_ARGS="-Xms128m – Xmx8192m ${MEM_DEV_ARGS} ${MEM_MAX_PERM_SIZE}"  This means that your intital heap size is set to 128 MB and max heap size is 8GB. Now , just save the file and restart the server. if it didn't resolve the issue, try increasing the size or look for ways to optimizing the service.  edit: Check whether you are able to see the updated java args while running the server . just like this If its coming as before then replace the shown value from setDoaminEnv.cmd by simple search and replace.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  