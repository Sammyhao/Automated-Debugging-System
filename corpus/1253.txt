 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I want to debug Map-reduce jobs (pig,hive) using eclipse. That is, to set break points in the hadoop source java file and to inspect the elements while running map-reduce jobs. To do this, I started all the services using eclipse and I can debug some class files. But I cant create an entire debug environment. Can anyone tell me how?  but...I started all the services using eclipse and I can debug some class files..but I cant create an entire debug environment  3 Answers 3  I do not know of an eclipse tool that can do what you are looking for. If you are looking for a possible solution the following will work in java.  For debugging java map reduce files you can use the java logger for each class( driver, mapper, reducer ).  To inspect elements/variables just use :  These log lines with be printed in the administration page for your job.  The basic thing to remember here is that debugging a Hadoop MR job is going to be similar to any remotely debugged application in Eclipse.  As you would know, Hadoop can be run in the local environment in 3 different modes :  Local Mode  Pseudo Distributed Mode  Fully Distributed Mode (Cluster)  Typically you will be running your local hadoop setup in Pseudo Distributed Mode to leverage HDFS and Map Reduce(MR). However you cannot debug MR programs in this mode as each Map/Reduce task will be running in a separate JVM process so you need to switch back to Local mode where you can run your MR programs in a single JVM process.  Here are the quick and simple steps to debug this in your local environment:  Run hadoop in local mode for debugging so mapper and reducer tasks run in a single JVM instead of separate JVMs. Below steps help you do it.  Configure HADOOP_OPTS to enable debugging so when you run your Hadoop job, it will be waiting for the debugger to connect. Below is the command to debug the same at port 8080.  Configure fs.default.name value in core-site.xml to file:/// from hdfs://. You won’t be using hdfs in local mode.  Configure mapred.job.tracker value in mapred-site.xml to local. This will instruct Hadoop to run MR tasks in a single JVM.  Create debug configuration for Eclipse and set the port to 8008 – typical stuff. For that go to the debugger configurations and create a new Remote Java Application type of configuration and set the port as 8080 in the settings.  Run your hadoop job (it will be waiting for the debugger to connect) and then launch Eclipse in debug mode with the above configuration. Do make sure to put a break-point first.  I got this to work following step 3 onward and making the following minor changes: step 3: simply change "hdfs://" to local as suggested here: wiki.apache.org/hadoop/HowToDebugMapReducePrograms. step 5. I did not configure the eclipse debug configuration to use a specific port. step 6: use debug mode in eclipse as usual - a breakpoint will be hit in the mapper function for each line in the input file.  I created an eclipse project to debug generic mapreduce program, for example WordCount.java, running standalone hadoop in Eclipse. But I did not try hive/pig specific mapreduce jobs yet. The project locates at https://github.com/drachenrio/hadoopmr, it can be download using  This project was created with Ubuntu 16.04.2, Eclipse Neon.3 Release (4.6.3RC2), jdk1.8.0_121, hadoop-2.7.3 environments.  Quick setup: 1) Once project imported into Eclipse, open .classpath, replace /j01/srv/hadoop-2.7.3 with your hadoop installation home path 2) mkdir -p /home/hadroop/input copy src/main/resources/input.txt to /home/hadoop/input/  It is ready to run/debug WordCount.java mapreduce job. Read README.md for more details. If you prefer to manually create the project, see my another answer in stackoverflow  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  