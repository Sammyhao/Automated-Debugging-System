 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  The problem, as you might have figured out from the code, is that I would like to be computed in parallel with whenever condition is true i.e. before is replace by new from . But according to my algorithm, this clearly can not happen due to the new b's.  I do think such a problem might be perfect for parallel computing approach. But, I did not use it before and my knowledge about that is quite limited. I did try the suggestion from How to do parallel programming in Python, though. But I got the same result like the sketch above.  Parallelizing this looks difficult because you don't want to spawn new threads for every recursive call -- if the recursion is deep you'd use tons of threads (way more than # CPUs) and none would be fast. Also I'm confused when you say "b is replaced by a new b". Each stack frame (recursive call) gets it's own in the statement -- whereas the list is the same globally, so all recursive calls will append to the same .  I see. Yeah, what I meant by new is the following: when condition is true , then if condition is again true, .... It's after completing this that it goes to . Therefore the only chance it has is to compute only where is the last by . But, I would like every to be computed for each 's i.e. . I am not sure if I explained the issue well.  ...So I've cheated/been-lazy & used a thread/process neutral term "job". You'll need to pick either threading or multiprocessing for everywhere that I use "job".  ...that will be best if you need a,b pairs to finish together for some reason. If you're willing to let the scheduler go nuts, you could "job" them all & then at the end:  But with multiprocessing you'd have more work to do with and a fixed size of workers  Thanks for detailed algos and options. Following your opinion to use multiprocessing (), I am not able to translate the 'job'-attached variables. 1. How could I find number_active_jobs() in mlt? or are you suggesting me to use thread as well? 2. What did you mean by ? if it is a variable, when is it updated for the last condition to make sense? 3. I also am too inexperienced in this 'mlt' module to understand . I checked but didn't know which one you were referring to.  NOTE: in order for x to be shared memory across the multiprocessing processes, would need to be a instead of a simple Python list. Answers: (1) After a bit more digging it looks like a fixed size pool would be easier than trying to count the forked jobs: (2) Ooops. I edit the code above to add the assignment that I forgot. (3) the equivalent of fork_job(func2, a, x) with a multiprocessing Pool would be something like `pool.apply_async(func2, [a, x]) (*) Lots more detail in the examples docs.python.org/2/library/multiprocessing.html  Hey Douglas. Thanks a lot. My fear was in fact pointless. The code by itself does the job as I wanted, just fine. But definitely, your feedback are great input for me cus my next step is just using multiple cores for which I need 'mlt' or 'th' that I have no enough knowledge of. Thanks a lot!!!  From your explanation I have a feeling that it is not that gets updated (which is not, as DouglasDD explained), but . To let both recursive calls to work on a same , you need to take some sort of a snapshot of . The simplest way is to pass an index of a newly appended tuple, along the lines of  7,23211 gold badge1111 silver badges2727 bronze badges  1  thanks, but DouglasDD understood my problem. Quite rankly, I also thought there might be a simple trick by some identifier to switch parameters for func2, similar to your opinion. I am not sure, though, if yours would work for me. Because a,b = func1(c) i.e. there is another function func1 to be called within func2.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  