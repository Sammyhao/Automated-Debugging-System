 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I know there's a similar question in stack overflow, where one person has asked, why time complexity of BFS/DFS is not simply O(V).  The appropriate answer given was that E can be as large as V^2 in case of complete graph, and hence it is valid to include E in time complexity.  But, if V cannot be greater than E+1. So, in that case not having V in the time complexity, should work?  2 Answers 2  If it is given that , for some real constants and then, and your argument is correct.  An example of this is trees.  In general (i.e., without any prior information), however, , and thus we cannot do better than .  Why not write just O(E) always?  EDIT: The primary reason for always writing is to avoid ambiguity.  For example, there might be cases when there are no edges in the graph at all (i.e. ). Even for such cases, we'll have to go to each of the vertex (), we cannot finish in time.  @Sandy only if the above conditions are satisfied. In general case, there might be no edges, still you'll have to go to each vertex, thus we write .  How would you "go to each vertex" when you can't find them via edges? This answer is incorrect.  V has to be included because both BFS and DFS rely on arrays of size |V| to track which vertices have been processed/discovered/explored (whatever the case may be). If a graph has 0 edges and 100000 vertices, such arrays will still take more time to initialize than they would if there were only 5 vertices. Thus, the time complexities of BFS and DFS scale on |V|.  You can implement BFS or DFS using a hashtable instead of an array, so that the initialisation takes O(1) time instead of O(V).  Do you mean inserting each vertex into the hash table (i.e. using it as an unordered map from a vertex to a boolean)?  The keys in the hashtable are the visited vertices; a vertex is visited if and only if it is present in the hashtable. That way you can start with an empty hashtable (i.e. no vertices are visited yet), and you don't have to mark each vertex as "not visited yet" explicitly. I was thinking of the hashtable as the implementation of an unordered set rather than a map, but if there are different states (e.g. pre-visited vs. post-visited), for example, then a map could be more useful.  @kaya3 how big is your hash table? Did you initialize it? :) OK OK, if you use a dynamically growing hash table then the complexity of BFS is expected O(E) not O(V+E). That's just not the usual implementation, and for good reason.  @MattTimmermans It is quite usual to implement BFS and DFS using a dynamically-growing hashtable if you are writing in a higher-level language where this is the data structure used for the built-in set type. For example, two of the top three Google results for 'depth first search python' initialise , links here and here.  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  