 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  This is a question that has been bugging for me a long time.  I have a huge dataset and I want to sort that dataset using a number that is generated each time a for loop is run throughout that dataset. So, I decided to bind that number to the index of a row in a map. This way, I have indirectly sorted the the dataset (which is a 2D vector of strings). Now I have been told there is a better way for me to do this, preferably by the heap data structure. But I do not know how a heap, like a priority queue would be helpful in this situation (except for being able to print out the smallest element in the heap which would take O(n) time). And I am strictly talking about time complexity here. I'll show you the code that I have:  How could a priority queue possible make it faster than what I did?  There are reasons why different containers exist. They have different properties, and different use cases. Comparing only the "time complexity" of each is like trying to make a meaningful comparison between apples and onions. One is not a complete substitute for the other, with the only difference being "time complexity".  Map is a node based data structure (such as a red-black tree), so inserting all your elements one at a time is quite expensive both in compute and memory. It's better to copy your elements to another vector and then do a . (Or + if you're really hung up on heaps for some reason.) Both methods can will work directly on the original data as well if it's OK to mutate it. As to your second question, let's hope the boss doesn't read Stack Overflow.  @Freexanman You will hopefully have a few internships beyond this one. All a part of being an undergrad, albeit it's quite weird you got one before even going through the basics such as data structure class which should have made it clear how to compare data structures and pick one that gets the job done.  1 Answer 1  Maps are typically implemented as binary search trees.  This implies that insertion of one key/value pair has a time complexity of O(log𝑛), where 𝑛 is the size of the map at the time of insertion.  To insert all key/value pairs you will thus have a total time complexity of O(log1 + log2 + log3 + ... + log𝑛) = O(log(𝑛!)) = O(𝑛log𝑛)  This is the same time complexity you get from storing the key/value pairs in a vector and sorting it by key calling .  Heap  A heap-based algorithm can have an advantage when you don't really need to sort the whole input, but are only interested in the least 𝑘 elements, or maybe even only the least 𝑘th element.  Inserting into a heap has also a time complexity of O(log𝑛), where 𝑛 is the size of the heap at the time of insertion.  However, a well designed heap-based algorithm will not need the heap to have more than 𝑘 elements: once that size is reached, values are exchanged -- also with O(log𝑘) time complexity. So the overall time complexity will then be O(𝑛log𝑘). This is definitely an improvement on O(𝑛log𝑛) when 𝑘 ≪ 𝑛.  242k2929 gold badges198198 silver badges234234 bronze badges  3  "This is the same time complexity" Same complexity, yes. And yet an of an with small K and V beats building a from scratch typically by leaps and bounds, as sorting a vector all at once yields a 10-100x lower real world cost due to significantly better data locality in a well chosen sorting algorithm compared to tree traversal.  @Ext3h, true. "with small K and V beats...": this is a practical consideration when designing an algorithm when you know the upper bounds to deal with. Time complexity is about asymptotic behaviour for increasingly large input parameters. And the Asker's question was specifically about "better...in terms of time complexity"  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  