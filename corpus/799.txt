 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  I have implemented Depth First Search algorithm in an iterative manner and a recursive manner. They both work fine on files with small sizes (less than 1 MB). However, when I try to run them over files with 50 MB, it seems like that the recursive-DFS (9 secs) is much faster than that using an iterative approach (at least several minutes). In fact, the iterative approach took ages to finish.  The only reason I chose to implement the iterative DFS is that I thought it may be faster than the recursive DFS.But this does not seem to be the case. Is this expected ?  Note that: I was already using to increase the memory.  Below is the code I used to write the iterative DFS.  Why are you constructing iterators for every single vertex? Why not just as you need them, storing them in the stack?  Because I want to be able to keep track of which vertex in each adjacency list needs to be explored next.  1 Answer 1  If you're talking about time complexity, the answer is that both of them are the same. Regardless of implementation, the runtime of DFS is O(V), because you should be visiting each vertex in the graph exactly once. Moreover, the runtime is also Omega(V), because you access all the vertices in the graph once regardless of input size. This leaves the runtime of DFS at Theta(V). The overall DFS algorithm remains the same regardless of implementation. Therefore, the runtime of recursive vs. iterative DFS should both be the same, at Theta(V).  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  