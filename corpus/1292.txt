 your communities  Find centralized, trusted content and collaborate around the technologies you use most.  Connect and share knowledge within a single location that is structured and easy to search.  The following code runs OK in VC9 but in VC10 throws a exception. The strange thing is that if I recover from the exception future allocations will succeed (the size of the container continues to grow). Also if I use it works fine in both compilers.  Regarding the actual memory usage, I'm running on a machine with 4GB RAM, (1.7 in use) the VC9 version gets up to ~810MB of memory before completing the task and the VC10 one crashes at ~658MB.  Is this a bug in VC10? I'm running on the same machine what else could cause memory to consistently run out in one version and not in the other when the amount of work done is identical?  <edit> Some more information: The first time the exception takes place is when calculating 7,718,688 with a stack depth of 1 (no recursion just main->length). After that it seems to happen for each number that is added to the cache. The cache had 16,777,217 elements in it before the exception happened (according to ). The interesting thing is that even when fails the cache size grows by one so it appears that it doesn't supply the strong exception guarantee (in violation of §23.2.1.11). </edit>  Code follows:  <edit> Also can anyone reproduce this behaviour, at one time it disappeared (and re-appeared) so there may be something special about my configuration. </edit>  @humbagumba I've added data about memory in the question, although, as I understand things, this isn't relevant since you only run out of memory if you're out of virtual memory not RAM.  I ran this on Linux and got a total non-memory-mapped size of 521,617,408 bytes. To that you need to add code+data from the executable, all the DLLs, and the stack. While the map is being resized the space required would be double that, which means that on a 32-bit system you could be getting close to the limit of virtual memory. Remember this is not limited so much by the size of the swap file as by the virtual address range available which I think is 2GB on 32-bit Windows.  I ran a test on WinXP that does a binary chop to find out the largest memory block that can be allocated (pastebin.com/WmWbjB6H). On my system, it was around 1.8GB. This is more than the amount of physical RAM, and it took ages to run because of all the swapping. On my 32-bit Linux machine with 8GB RAM it runs instantly (because of the use of mmap) and returns a limit of 2.94GB. On my 64-bit Mac it took about a minute but returned a limit of 140TB :-O  5 Answers 5  It might be incidental, but changing the value of _SECURE_SCL causes the behavoir you describe.  i.e Compiling with:  crashes, but the same commands with _SECURE_SCL=0 runs to completion on my XP 32bit machine. The msdn page for _SECURE_SCL says it's enabled for debug builds, but not release, which might be important if you're building under the IDE.  4,39633 gold badges2626 silver badges2121 bronze badges  2  I'll look into it although I've been building with the default release flags from the IDE.  _SECURE_SCL adds a lot of debugging checks to STL code, so it would cause memory usage to increase considerably. (See other comments.)  Inserting a single element could result in a large memory allocation if the map's hash table needs to be resized. The map seems to be about 0.5GB at the end of the run. (See my comment above.)  There is presumably some heuristic being used to decide how much to expand the hash table when it needs to grow, and this could conceivably be to double it each time. This would therefore use ~1.5GB for old + new data while the hash table is being copied.  It's therefore possible your program is hitting a limit on process memory size. (See comment again.) If so, it's possible that VC10 takes slightly more memory overall than VC9, and that slightly different amounts of memory get allocated on different runs or builds of the program, so that VC10 hits the limit sometimes while VC9 doesn't ever hit it.  13.1k11 gold badge3030 silver badges2323 bronze badges  6  Are you sure that resizing an unordered_map should mean copying all the memory? AFAIK an unordered_map is not contiguous and resizing it should not be dependant on the amount of data currently in the map.  Yes, this occurred to me too, some time after I'd written this answer. However, I've checked the documentation for unordered_map on MSDN and there is still a hash table involved. It groups the map elements into buckets, and each bucket is effectively a std::list. The buckets are stored in a hash table. So the unordered_map implementation will need to grow that table as the map grows, although the amount of memory reallocation won't be as much as I said. There are several methods available for checking things like the number of buckets, so you could use these to investigate the storage pattern.  You could define a custom allocator and use it with cache_type to trace the memory allocations and deallocations. In particular, you could find out the size of the memory allocation that fails. To avoid changing the problem behaviour you could implement the custom allocator as a decorator for the standard allocator.  It would also be helpful to know the value of n when cache.insert throws, so see how far through the problem the program is when it fails. If it's very close to the end this might help to explain why VC9 makes it all the way and VC10 doesn't.  I've updated my question with some more information, BTW when you comment on your own answer I don't get notified unless you put my handle in the comment (@Motti).  Does _int64 have alignment requirements that the map may not be honoring in allocation?  Try using long long int instead and see if the behavior changes.  Interesting problem, and certainly seems like a VC10 regression. All this code is in the same compilation unit (no DLL boundaries?) It does sound like a heap allocation failure, but just to rule it out, can you crank up the stack reserve size in the link settings? That is a default that may have changed (or stack behavior may be different with new VC10 features) Do you know the make_pair or insert is throwing (which one?) Good luck!  The code is as you see it, one exe no DLL's no different translation units for the linker. The exception takes place in which makes sense since doesn't allocate any memory. As for changing the stack reserve size, I tried that and got strange results. The exception went away, I changed it back the the default and it didn't re-appear, then I rebooted the machine and the problem returned (both with the default 1MB stack and 2MB)...  Wow - strange stuff. Only other things I'd try would be some of the heap allocation flags, and/or try a 64-bit build if it's currently 32. Sounds like an MS bug based on what you've seen so far, so you might check the msdn boards.  It would be better to include <cstdint> and use int64_t because _int64 is Microsoft-only but int64_t is standard.  1 - Check EventLog to see if there are any events talking about a process going over it's allowed quota.  2 - If you are on 32-bit OS try starting it with 3GB for user space.  3 - Look to see if you have different allocators available  4 - Diff unordered_map in 9.0 and 10.0 and it's in-lined file on on off-chance that there's an artificial size limiter added ("security features" :-). It would most probably be in a macro with different values for x86 and x64 build.  5 - Try to put a light wrapper around the allocator and just print sizes for each allocation. That will also tell you if it's really allocator that's throwing or something before it.  6 - If it's allocator throwing look at actual WinNT API calls made from it (and diff with 9.0 again)  I don't think you're right, I'm getting a exception and the stack depth is the same for VC10 and VC9  You're right. The recursion depth maxes out at about 400 for the given input. The cache size is about 22 million items. boost::unordered_map appears to have an overhead of roughly 16 bytes per item for this (28 bytes per item total), so you're looking at a cache footprint of about 608 MB. Would you abort() or break at the point of the bad_alloc, and tell us what the cache size, stack size, stack depth, heap usage, etc., are?  Hi John I've updated my question with some more information, BTW when you comment on your own answer I don't get notified unless you put my handle in the comment (@Motti).  By clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.  